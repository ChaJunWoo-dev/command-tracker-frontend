## 📌 프로젝트 소개

command-tracker는 스트리트파이터라는 1대1 격투 게임 영상에서 플레이어의 커맨드 입력을 자동 추출해 주는 서비스입니다.

일반적으로 커맨드가 영상에 포함되지 않은 경우, 녹화된 영상을 보며 수동으로 입력을 추정해야 하며, 이는 초보자에게 높은 진입장벽이 됩니다.

이 서비스는 RTMPose 기반의 관절 추출 모델과 후처리 로직을 활용하여, 플레이어의 동작을 분석하고 이를 커맨드로 변환, 최종적으로 자막 형태로 영상에 삽입해 줍니다. 이로써 초보자들도 원하는 캐릭터의 커맨드를 손쉽게 확인해 볼 수 있습니다.

> RTM-Pose는 사람의 관절 위치를 추정해주는 최신 인체 포즈 추정 AI 모델입니다.


## 🔗 링크  

<div align="center">
  
[웹사이트](https://commandtracker.co.kr/) | [웹 서버 레포지토리](https://github.com/ChaJunWoo-dev/command-tracker-backend) | [AI 레포지토리](https://github.com/ChaJunWoo-dev/command-tracker-ai)

</div>


## ⚙️ 레포지토리 구성 및 역할

### 클라이언트 레포지토리
- 사용자가 분석할 유튜브 영상 주소를 입력하고 커맨드를 추출할 부분을 컷 편집할 수 있는 웹 UI 제공합니다.
- 사용자가 영상을 제출하고, 결과를 받을 이메일을 입력하는 인터페이스 제공합니다.
- 주요 기능
  - 유튜브 영상 링크 입력 UI
  - 이메일 입력 및 제출

### 웹 서버 레포지토리
- 클라이언트의 요청을 받아 저장하고, 작업 요청을 RabbitMQ로 발송하거나 결과를 수신하여 이메일로 최종 영상을 전송하는 Express API 서버입니다.
- 클라이언트와 AI 서버를 직접 연결하지 않고, 역할을 분담하여 유지보수성과 안정성을 증가. 이를 위한 중계 서버 역할을 합니다.
- 주요 기능
  - Youtube 영상 다운로드 처리
  - AI 서버로 작업 요청 메시지를 RabbitMQ에 발행
  - 추출 완료된 최종 영상의 URL을 받아 이메일로 전송
 
### AI 레포지토리
- RabbitMQ로부터 수신한 작업 요청을 기반으로, RTM-pose로 관절 추출 및 커맨드 인식을 수행하는 Python 서버입니다.
- 고성능 모델을 실행하고 비동기 큐 기반으로 독립된 연산 서버가 필요하여 별도로 분리된 서버입니다.
- 주요 기능
  - RTM-pose로 관절 위치 추정
  - 관절 위치 정보를 기반으로 커맨드 추정
  - 추정 결과를 자막에 담아 최종 영상 URL을 메시지 큐로 전송


## **🛠** 기술 스택

### 클라이언트

<span>
  <img src="https://img.shields.io/badge/javascript-F7DF1E?style=for-the-badge&logo=javascript&logoColor=black">
  <img src="https://img.shields.io/badge/react-61DAFB?style=for-the-badge&logo=react&logoColor=black">
  <img src="https://img.shields.io/badge/vite-646CFF?style=for-the-badge&logo=vite&logoColor=white">
  <img src="https://img.shields.io/badge/tailwindcss-06B6D4?style=for-the-badge&logo=tailwindcss&logoColor=white">
</span>

### 웹 서버

<span>
  <img src="https://img.shields.io/badge/Node.js-339933?style=for-the-badge&logo=nodedotjs&logoColor=white">
  <img src="https://img.shields.io/badge/Express-000000?style=for-the-badge&logo=express&logoColor=white">
</span>

### AI 서버

<span>
  <img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white">
  <img src="https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white">
</span>

### 서버 공통

<span>
  <img src="https://img.shields.io/badge/RabbitMQ-FF6600?style=for-the-badge&logo=rabbitmq&logoColor=white">
  <img src="https://img.shields.io/badge/FFmpeg-007808?style=for-the-badge&logo=ffmpeg&logoColor=white">
</span>

### 배포

<span>
  <img src="https://img.shields.io/badge/vercel-E34F26?style=for-the-badge&logo=vercel&logoColor=black">
  <img src="https://img.shields.io/badge/amazonaws-232F3E?style=for-the-badge&logo=amazonaws&logoColor=black">
</span>

### React 도입 배경

이 프로젝트는 영상 업로드부터 컷 편집, 분석 요청까지 단계가 명확히 구분됩니다. React의 SPA 구조를 활용해 페이지 전환 없이 필요한 부분만 갱신함으로써 사용자 경험을 매끄럽게 유지하고자 했습니다. 또한 컴포넌트 단위 개발을 통해 수정과 확장 시 수정 범위를 최소화하여, 유지보수성을 높이고자 했습니다. 

## 🧠기술적 챌린지

### 영상 컷 편집

사용자가 입력한 유튜브 링크에서 원하는 구간만 잘라 커맨드 추정에 사용하는 기능이 필요했습니다. 유튜브 영상의 길이는 다양하고, 전체 영상을 처리하기에는 자원 낭비가 크기 때문에 컷 편집은 필수였습니다. 처음에는 `ffmpeg`를 사용하여 유튜브 스트림을 그대로 pipe 처리하였고, 출력 확장자는 일반적으로 많이 사용하는 `.mp4`를 선택했습니다. 그러나 이 방식에서는 영상이 저장되지 않거나, 저장되더라도 영상이 깨지거나 디코딩 오류가 발생하는 문제가 반복되었습니다.

초기에는 스트림 처리 방식 자체에 문제가 있다고 판단하여, 임시 디렉터리에 파일로 저장한 후 편집을 수행해 보았습니다. 이 방식은 안정적으로 동작했지만, 사용자 요청마다 서버에 영상 파일이 남는 문제가 있었습니다. 이에 문제의 근본 원인을 찾기 위해 확장자를 조사하였고, 스트리밍 환경에 적합한 포맷인 `.webm`의 존재를 확인했습니다. 실험 결과 `.webm` 확장자를 사용할 때 pipe 스트리밍 방식에서도 영상이 안정적으로 저장되고 편집되는 것을 확인하였고, 이를 최종 구조로 채택하였습니다.

현재는 Node.js 서버에서 클라이언트로부터 전달받은 시작 점과 종료 점을 기반으로 `ffmpeg` 명령어를 실행하고, `.webm` 포맷으로 출력된 편집 영상 파일을 Google Cloud Storage 버킷에 업로드하고 있습니다. 

개선해야 할 점은 영상 편집에 대한 문제입니다. 현재 컷 편집에만 수십 초가 소요되며, 이에 따라 사용자에게 응답이 늦어지는 문제가 있습니다. 이를 개선하기 위해서는 해상도를 낮춰야 하지만, 해상도를 낮추는 건 사용자 측면과 AI 영상 분석 성능에 악영향을 끼치기 때문에, 다음과 같이 해결할 계획입니다. 요청과 동시에 성공 응답을 먼저 반환하고, 이후 내부적으로 컷 편집과 분석을 진행하여 UI적으로 해결할 예정입니다.


### AI와 커맨드 추정하기

게임 속 캐릭터의 커맨드를 추정하기 위해서는, 먼저 해당 캐릭터가 어떤 자세를 취하고 있는지를 AI가 인식할 수 있어야 했습니다. 그리고 AI 모델을 활용하기 위해서는, 해당 객체가 사람이라는 전제하에 관절을 추정할 수 있어야 했습니다. 즉, 분석 과정은 객체 식별 → 관절 추정 → 커맨드 분류의 흐름으로 구성됩니다.

하지만 대상이 실제 인물이 아닌 게임 캐릭터였기 때문에 문제가 발생했습니다. 사람으로는 불가능한 과장된 동작을 취하거나, 외형 자체가 일반적인 사람과는 달라서 관절 추정 정확도가 낮아지는 경우가 빈번했습니다.

이를 해결하기 위해 초기에 팀원과 함께 직접 라벨링 및 학습을 수행했고, 약 3,000장의 학습 데이터를 수집해 하나의 캐릭터에 대해 객체 식별이 가능하도록 학습시켰습니다. 이 방식은 외형이 고정된 상태에서는 높은 정확도를 보였지만, 스킨 변경이나 다른 캐릭터에 대해서는 인식률이 저하되었고, 전체 게임 다양성을 고려할 때 확장성에 큰 제약이 있었습니다.

결국 범용성과 유지보수를 고려해, 캐릭터를 좌측/우측으로 단순히 구분하는 구조로 전환하였습니다. 게임이 1:1 구조이기 때문에, 사용자가 분석 요청 시 좌측 캐릭터인지, 우측 캐릭터인지 선택하면 이를 기준으로 후속 AI 분석이 진행됩니다.

관절 추정에는 `RTMPose`를 사용했습니다. 해당 모델은 프레임 단위로 2D 관절 좌표(x, y)를 출력하며, 추출된 데이터를 바탕으로 특정 커맨드를 판단하는 후처리 로직을 구현하였습니다.

예를 들어 팔을 뻗는 동작은 어깨–팔꿈치–손목의 각도를 계산하고, 해당 각도가 일정 범위에 들어오면 조건문 기반 분기 처리로 커맨드를 판별하는 구조입니다.

다만 개선이 필요한 부분도 존재합니다. 현재는 좌측/우측 캐릭터를 사용자가 수동으로 선택해야 하며, 플레이 도중 캐릭터의 위치가 바뀔 때는 대응이 어렵습니다. 기술적으로는 프레임 단위로 캐릭터의 위치 변화를 추적하거나, 이전처럼 객체 라벨링을 통한 ID 매칭 로직을 적용하는 방법도 있습니다. 하지만, 이 방식은 개발 자원이 크고, 처리 시간과 정확도가 높지 않다는 점에서 모두 위험이 크기 때문에 실제 서비스에 적용하기에는 부담이 크다고 판단 했습니다. 따라서 현재는 UI에서 사용자가 명확하게 좌/우를 선택하도록 하는 방식으로 문제를 해결하고 있습니다. 

또한 커맨드 판별은 각도 기반 조건문 기반으로 처리되며, 캐릭터마다 30개 이상의 동작을 직접 정의해야 하는 구조입니다. 이 영역은 자동화하거나 템플릿화하기 어려운 특성상, 캐릭터가 추가되거나 동작이 바뀔 때마다 직접 커맨드 분기 로직을 추가해 줘야 합니다. 따라서 유지보수 비용은 계속 발생하지만, 그만큼 커맨드 동작의 판별 기준을 정밀하게 조정할 수 있는 유연성도 함께 확보할 수 있습니다.

> RTM-Pose는 사람의 관절 위치를 추정해주는 최신 인체 포즈 추정 AI 모델입니다.


### 영상에 커맨드 넣기

커맨드 추정이 완료되면, 캐릭터의 동작과 함께 해당 커맨드를 사용자도 확인할 수 있도록 자막을 생성하고, 이를 영상에 삽입하는 과정을 거칩니다. 이때 자막은 특정 시간 구간에만 표시되는 방식이 아니라, 스택형 자막 방식으로 구현하여 이전 자막 위에 새로운 커맨드가 누적되어 쌓이는 형태로 처리됩니다. 이러한 방식은 1:1 격투 게임의 연습 모드에서 흔히 사용되는 입력 커맨드 표기 형태를 그대로 빌린 것으로, 사용자에게 익숙한 형태로 커맨드를 보여줄 수 있다는 장점이 있습니다.

자막 포맷은 `.ass`를 사용하였습니다. 흔히 사용되는 `.srt`와 달리 `.ass`는 정밀한 자막 위치 제어 및 스타일링이 가능하므로, 스택형 자막처럼 복잡한 자막 구성에 적합하다고 판단했습니다. 최종적으로는 ffmpeg를 사용하여 자막이 삽입된 영상을 렌더링하고, 완성된 영상은 Google Cloud Storage 버킷에 업로드됩니다. 이후 사용자에게는 signed URL을 발급하여, 24시간 동안 영상 결과물을 확인할 수 있도록 구현하였습니다.

현재 구현된 스택형 자막 방식을 사용하면 사용자에게 익숙한 형태로 보여줄 수 있다고 생각했으나, 실제로 확인해 본 결과, 실제 게임 내 커맨드 입력 UI와의 차이로 오히려 불편하게 느껴질 수 있다고 생각했습니다. 자막이 단순 텍스트 형태로 누적되어 쌓이기 때문에 몰입감이 떨어지고, 커맨드 흐름도 깔끔하게 전달되지 않는 한계가 있었습니다. 이를 개선하기 위해 일반적인 자막처럼 표시 구간을 지정하고, 특정 시간 범위에만 자막이 등장하고 사라지는 구간형 자막 방식으로 변경할 계획입니다. 이를 통해 자막이 자연스럽게 나타나며, 사용자 경험도 보다 향상될 것으로 기대하고 있습니다.


## 🔍 작업 방식

- **깃 브랜치 전략**
    
Github Flow 전략을 채택했습니다. Git Flow는 develop, release 등 브랜치가 많아서 관리가 복잡하고 작은 팀에서는 오히려 비효율적이라 생각했습니다. 반면 Github Flow는 main 브랜치를 기준으로 병합하기 때문에 빠른 개발 사이클을 가져갈 수 있어 개발 기간을 확보할 수 있다고 생각하여 적합하다고 판단했습니다. 
    
- **PR 규칙**
    
PR 작성 시 이슈 번호를 반드시 포함하고 템플릿을 지켜 작성하였습니다. 내용에는 변경된 내용과 코드 리뷰어가 확인해야 할 부분을 간단히 명시하였습니다. 또한 팀원의 승인이 반드시 있어야 병합할 수 있도록 하여 실수로 인한 병합을 예방하였습니다.
    
- **협업 시 신경 썼던 부분**
    
감정을 배제한 의사소통을 유지했고, 오해 소지가 있는 표현은 즉시 정정했습니다. 변수명은 길더라도 명확하게 작성했으며, 수정 사항은 회의 때 공유 후 합의해 진행했습니다. 필요한 경우 참고 자료를 함께 제공해 소통 효율을 높였습니다.
    

## 📆 일정 및 팀원

진행 기간 : 5주

팀원 : 차준우, 조성경 (총 2명)

- 1주차
    - 아이디어 선정
    - POC 진행 - 캐릭터의 커맨드를 추정하기 위한 관절 데이터 추출 성능 확인
- 2주차
    - POC 진행 - 캐릭터의 커맨드를 추정하기 위한 관절 데이터 추출 성능 확인
    - 칸반 작성
    - 기술 스택 조사
- 3주차
    - 프로젝트 환경 세팅
    - 공통 컴포넌트 구현
    - 영상 편집 구현
    - 영상 업로드 구현
    - 결과물 확인 링크 메일 전송 구현
- 4주차
    - 영상 분석하여 관절 데이터 추출 구현
    - Re-ID 성능 테스트
    - 동작 학습 데이터 수집 및 AI 기반 커맨드 추출 테스트
    - 관절 데이터 기반 커맨드 추출 구현
- 5주차
    - 배포


## 💭 개인 회고

이번 프로젝트를 통해 단순히 기능을 구현하는 것을 넘어, 실제 사용자가 어떤 문제를 겪고 있는지를 정의하고, 그에 맞는 해결책을 설계하는 것이 얼마나 중요한지 체감했습니다. 또한, AI 기반 관절 추정을 처음에는 캐릭터별 학습 방식으로 구현했으나, 매번 라벨링·학습을 반복해야 해 유지보수 한계가 컸습니다. 이를 좌/우 캐릭터 구분 방식으로 전환하며, 기술적 완벽함보다 현실적 확장성을 우선하는 선택이 더 효율적일 수 있음을 배웠습니다.

### 아쉬운 점

AI 기반 커맨드 추정의 정확도에 대한 확신이 부족해, 필요한 수준보다 지나치게 완벽한 결과를 목표로 POC에 과도한 시간을 투입했습니다. 그 결과 설계가 지연되어 UX 개선과 코드 정리에 충분히 집중하지 못했습니다. 앞으로는 목표 수준을 현실적으로 설정하여 프로젝트 진행 속도를 높일 것입니다.

### 향후 개선 방향

- 커맨드 조건 정교화로 정확도 향상
- 스택형 자막 → 구간형 자막 개선
- 영상 처리 구조를 비동기화해 UX 개선
